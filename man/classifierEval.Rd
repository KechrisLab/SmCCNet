% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AutoSmCCNet.R
\name{classifierEval}
\alias{classifierEval}
\title{Evaluation of Classifier with Different Evaluation Metrics}
\usage{
classifierEval(
  obs,
  pred,
  EvalMethod = "accuracy",
  BinarizeThreshold = 0.5,
  print_score = TRUE
)
}
\arguments{
\item{obs}{Observed phenotype, vector consists of 0, 1.}

\item{pred}{Predicted probability of the phenotype, vector consists of any value between 0 and 1}

\item{EvalMethod}{Binary classifier evaluation method, should be one of the following:
'accuracy', 'auc', 'precision', 'recall', and 'f1', default is set to accuracy.}

\item{BinarizeThreshold}{Cutoff thresold used to binarize the predicted probability, default is set 
to 0.5.}

\item{print_score}{Whether to print the evaluation score or not, default is set to 0.5.}
}
\description{
evaluate binary classifier's performance with respect to user-selected
metric (accuracy, auc score, precision, recall, f1).
}
\examples{
# simulate observed binary phenotype
obs <- rbinom(100,1,0.5)
# simulate predicted probability
pred <- runif(100, 0,1)
# calculate the score
pred_score <- classifierEval(obs, pred, EvalMethod = 'f1', print_score = FALSE)

}
